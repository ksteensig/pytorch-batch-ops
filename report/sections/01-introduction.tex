\chapter {Introduction}

The Singular Value Decomposition (SVD) is an essential tool in scientific computing. As computational problems have gotten more complex, many fields have begun applying scientific computing. These fields include image processing, machine learning, data analysis and dynamical systems. The SVD has important uses in all these fields. In image processing the SVD can be used for noise reduction, in machine learning it can be used for dimensionality reduction, in data analysis it can be uses for low-rank approximations and in dynamical systems it can be used to compute the pseudo-inverse for simulations. Many computational problems use large amounts of data that requires a lot of computation power to analyze. There is a constant demand to decrease computation time and data complexity in these systems. Because the SVD is so widely used, even a small improvement decrease in computation time can save a lot of computational resources.

A recent example of how the SVD keeps getting more useful is in the field of Deep Neural Networks (DNN), where it is possible to determine the complexity of functions a DNN can represent by using singular values. A DNN is a function $\hat y_i = f(x_i; \theta)$ with predicted value $\hat y_i \in \mathbb{R}^{d_{out}}$, input $x_i \in \mathbb{R}^{d_{in}}$ and parameters $\theta$. To train a DNN, i.e. estimating $\theta$, a dataset with inputs and matching labels is used. Such a dataset contains inputs $x_i$ and labels $y_i$ as in \eqref{eq:wang:data}.

\begin{equation} \label{eq:wang:data}
  \begin{split}
    D &= {(x_i,y_i)},\ i=1,\dotsc,N \\
    X &=
    \begin{bmatrix}
      x_1 & \cdots & x_N
    \end{bmatrix}^T \in \mathbb{R}^{N \times d_{in}}
  \end{split}
\end{equation}

Measuring the complexity of functions a DNN can represent is done using a method by Wang, et al. They refer to this measure as the \textit{score} of a DNN. The score was specifically created with feed-forward DNNs and convolutional DNNs in mind \cite{icml16:wang:edjm}. There are multiple steps to compute the score, the first of these is to compute the Data Jacobian Matrix (DJM) \cite{icml16:wang:edjm}. Given an input and predicted value from a DNN, it is possible to compute the DJM as \eqref{eq:wang:djm}.

\begin{equation} \label{eq:wang:djm}
    \mathrm{DJM}_{\theta}(x_i) = \frac{\partial \hat y_i}{\partial x_i},\ i=1, \dots, N
\end{equation}

From the computed DJMs the Extended Data Jacobian Matrices (EDJM) can then be constructed. An EDJM is constructed by combining all the rows at the same indices in the DJMs. As the DJMs have $d_{out}$ rows, it results in $d_{out}$ EDJMs, with the first EDJM containing the 1st row from all the DJMs, another containing the 2nd row from all the DJMs, and so on.

\begin{equation} \label{eq:edjm}
\mathrm{EDJM}_{\theta}(X, j) =
\begin{bmatrix}
\mathrm{DJM}_{\theta}(x_1)_j \\
\vdots \\
\mathrm{DJM}_{\theta}(x_N)_j
\end{bmatrix},\ j=1,\dots,d_{out}
\end{equation}

For each EDJM the \textit{score} is calculated as \eqref{eq:score}. As many of the normalized singular values are extremely small, they are discarded if they are below the relative threshold $\epsilon$. Wang, et. al. uses a relative $\epsilon$ that discards the 90\% smallest singular values.

\begin{equation} \label{eq:score}
  \begin{split}
  S(X, j) &=\ \{\sigma\ |\ \sigma \in \mathrm{singular\ values\ of\ EDJM}_{\theta}(X, j) \} \\  
  \mathrm{score}(S(X, j)) &=  \sum_{\sigma \in S(X, j), \sigma > \epsilon} \frac{\sigma}{\max{\sigma}}  
  \end{split}
\end{equation}

The score is similar to the nuclear norm $||X||_* = \sum_{\sigma \in S(X, j)} \sigma$, but instead of adding all the singular values, it is a normalized sum of singular values larger than $\epsilon$. For $\epsilon=0$ the score is exactly the normalized nuclear norm. Table \ref{tab:dnn:score} shows how changing the amount of layers while keeping hidden units constant works for four DNN configurations. Using the MNIST dataset \cite{lecun:mnist}, the DNNs are all trained until they reach approximately the same accuracy to ensure the accuracy does not contribute to the score. The DNNs are all feed-forward DNNs using Rectified Linear Unit (ReLU) activation functions. All layers in a DNN have the same number of hidden units. It is important to note that \textit{a higher score indicates a DNN is able to represent more complex functions}.

\begin{table}[H]
  \centering
    \begin{tabular}{|l|l|l|l|} \hline
      Hidden layers & Units per hidden layer & Accuracy & Score \\ \hline
      1 & 6144 & 95.95\% & 2.7092 \\ \hline
      2 & 3072 & 95.95\% & 2.8556 \\ \hline
      3 & 2048 & 96.03\% & 3.1424 \\ \hline
      4 & 1536 & 95.86\% & 3.3551 \\ \hline
    \end{tabular}
    \caption{Four DNN configurations trained on the MNIST dataset}
    \label{tab:dnn:score}
  \end{table}



\section{Problem Statement}

This project seeks to solve the problem of \textit{how can singular value decomposition be parallelized?} As it is too large and general of a task to solve this general problem of parallelized SVD in a semester project, a less general problem is to be solved. This less general problem to be solved is to compute the singular values of a feed-forward DNN using ReLU activation functions, which can then be used to compute the score. The problem with computing the score is that the problem grows larger with the amount of data, input data size and output data size. To solve this less general problem the problem statement is broken down into three subproblems.

\begin{itemize}
\item Which algorithms can be used to compute an SVD in parallel?
\item How can the the time complexity w.r.t. amount of data ($N$), data input size ($d_{in}$) and data output size ($d_{out}$) be reduced?
\item Which type of computing platform is suitable for implementation?
\end{itemize}

\noindent To try and answer these three questions the theory behind scientific computing, the SVD and a protential solution are analyzed.
